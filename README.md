# üï∑Ô∏è Web Crawler en Python

Un rastreador web sencillo pero potente que navega autom√°ticamente por p√°ginas web, extrae contenido relevante y sigue enlaces dentro del mismo dominio.

## üìã Requisitos

- Python 3.7 o superior
- pip (gestor de paquetes de Python)

## üöÄ Instalaci√≥n

1. **Clona o descarga el repositorio:**

```bash
git clone https://github.com/tu-usuario/web-crawler.git
cd web-crawler
```

2. **Instala las dependencias:**

```bash
pip install requests beautifulsoup4
```

O si prefieres usar un entorno virtual:

```bash
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# venv\Scripts\activate   # En Windows
pip install requests beautifulsoup4
```

## üíª Uso

### Uso B√°sico

1. Abre el archivo `web_crawler.py` y modifica la URL de inicio:

```python
START_URL = "https://tu-sitio-web.com"
```

2. Ejecuta el script:

```bash
python web_crawler.py
```

### Uso como M√≥dulo

Puedes importar la clase `WebCrawler` en tus propios scripts:

```python
from web_crawler import WebCrawler

# Crear instancia del crawler
crawler = WebCrawler(
    start_url="https://ejemplo.com",
    max_pages=50,    # M√°ximo de p√°ginas a visitar
    delay=1          # Segundos entre cada request
)

# Ejecutar el crawling
results = crawler.crawl()

# Guardar resultados en un archivo
crawler.save_results(results, filename="mis_resultados.txt")

# Procesar resultados manualmente
for page in results:
    print(f"T√≠tulo: {page['title']}")
    print(f"URL: {page['url']}")
    print(f"Headings: {page['headings']}")
    print(f"Preview: {page['preview']}")
```

## ‚öôÔ∏è Par√°metros de Configuraci√≥n

| Par√°metro | Tipo | Default | Descripci√≥n |
|-----------|------|---------|-------------|
| `start_url` | str | - | URL inicial desde donde comenzar el crawling |
| `max_pages` | int | 50 | N√∫mero m√°ximo de p√°ginas a rastrear |
| `delay` | float | 1 | Tiempo de espera (en segundos) entre requests |

## üìÅ Estructura de los Resultados

El crawler extrae la siguiente informaci√≥n de cada p√°gina:

```python
{
    'url': 'https://ejemplo.com/pagina',
    'title': 'T√≠tulo de la P√°gina',
    'preview': 'Primeros 200 caracteres del contenido...',
    'headings': ['Heading 1', 'Heading 2', 'Heading 3']
}
```

## üìÑ Archivo de Salida

Los resultados se guardan en `crawler_results.txt` con el siguiente formato:

```
================================================================================
P√ÅGINA 1
================================================================================

URL: https://ejemplo.com
T√≠tulo: P√°gina de Ejemplo

Headings: Bienvenido, Servicios, Contacto

Vista previa:
Este es un ejemplo del contenido de la p√°gina...
```

## ‚ö†Ô∏è Consideraciones √âticas

- **Respeta el archivo `robots.txt`** de los sitios web
- **No sobrecargues los servidores** - usa un delay adecuado
- **Verifica los t√©rminos de servicio** del sitio antes de crawlear
- Este crawler est√° dise√±ado para **uso educativo y personal**

## üîß Caracter√≠sticas

- ‚úÖ Crawling limitado al mismo dominio (evita salir del sitio)
- ‚úÖ Detecci√≥n y eliminaci√≥n de URLs duplicadas
- ‚úÖ Manejo de errores robusto
- ‚úÖ User-Agent configurable para evitar bloqueos
- ‚úÖ Delay configurable entre requests
- ‚úÖ Extracci√≥n de t√≠tulos, headings y contenido

## üìù Ejemplo de Ejecuci√≥n

```bash
$ python web_crawler.py

Iniciando crawling desde: https://example.com
M√°ximo de p√°ginas: 20

[1] Crawleando: https://example.com
[2] Crawleando: https://example.com/about
[3] Crawleando: https://example.com/contact
...

Crawling completado. Total de p√°ginas crawleadas: 20
Resultados guardados en: crawler_results.txt

Resumen:
- URLs visitadas: 20
- Primeras 5 p√°ginas crawleadas:
  1. Example Domain...
  2. About Us...
  3. Contact...
```

## üìú Licencia

Este proyecto est√° bajo la licencia MIT. Si√©ntete libre de usarlo y modificarlo.

